{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-000",
   "metadata": {},
   "source": [
    "# Accessibility Audit — End-to-End Pipeline Walkthrough\n",
    "\n",
    "**Two sequential passes over a real-world HTML page against WCAG criteria**\n",
    "\n",
    "---\n",
    "The audit runs in two passes:\n",
    "\n",
    "| Pass | Scripts | Purpose |\n",
    "|---|---|---|\n",
    "| **Pass 1 — Programmatic** | `programmatic/*.py` — 43 rules | Binary pass/fail: objectively verifiable violations reported immediately |\n",
    "| **Pass 2 — LLM** | `llm_preprocessing/*.py` + `llm/*.txt` — up to 21 calls | Quality judgment on items that *passed* Pass 1 |\n",
    "\n",
    "**Design principle**: Pass 1 results **filter** Pass 2 inputs. Items that failed a binary\n",
    "check (no alt attribute, no label, broken reference) are excluded from LLM evaluation —\n",
    "there is no value in judging the quality of something that doesn't exist.\n",
    "\n",
    "The Programmatic checks run in milliseconds and the overhead of running them first is negligible. This provides filtered LLM payloads,\n",
    "no duplicate findings, and lower token cost.\n",
    "\n",
    "---\n",
    "\n",
    "**Test page**: [Vision Aid](https://visionaid.org) homepage — 1.9 MB, 14,626 lines of WordPress/Elementor HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML file    : /Users/andrew/git/visionaid-a11y-llm-audit/test_files/home.html  (exists=True)\n",
      "Programmatic : /Users/andrew/git/visionaid-a11y-llm-audit/processing_scripts/programmatic\n",
      "LLM preproc  : /Users/andrew/git/visionaid-a11y-llm-audit/processing_scripts/llm_preprocessing\n",
      "Prompts      : /Users/andrew/git/visionaid-a11y-llm-audit/processing_scripts/llm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, re, sys, importlib.util\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "# ── Paths (notebook lives at processing_scripts/) ─────────────────────────────\n",
    "NOTEBOOK_DIR  = Path(\".\").resolve()\n",
    "PROJECT_ROOT  = (NOTEBOOK_DIR / \"..\").resolve()\n",
    "HTML_FILE     = PROJECT_ROOT / \"test_files\" / \"home.html\"\n",
    "PROG_DIR      = NOTEBOOK_DIR / \"programmatic\"\n",
    "PREPROCESSING = NOTEBOOK_DIR / \"llm_preprocessing\"\n",
    "PROMPTS_DIR   = NOTEBOOK_DIR / \"llm\"\n",
    "\n",
    "def load_module(name, directory):\n",
    "    path = directory / f\"{name}.py\"\n",
    "    spec = importlib.util.spec_from_file_location(name, path)\n",
    "    mod  = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "print(f\"HTML file    : {HTML_FILE}  (exists={HTML_FILE.exists()})\")\n",
    "print(f\"Programmatic : {PROG_DIR}\")\n",
    "print(f\"LLM preproc  : {PREPROCESSING}\")\n",
    "print(f\"Prompts      : {PROMPTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-002",
   "metadata": {},
   "source": [
    "---\n",
    "## The Problem — Raw HTML Is Too Large for Direct LLM Processing\n",
    "\n",
    "Before running the audit, let's see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================\n",
      "  RAW HTML FILE METRICS\n",
      "========================================================\n",
      "  File size    :  1,948,296 bytes  (1.86 MB)\n",
      "  Characters   :  1,947,379\n",
      "  Lines        :     14,626\n",
      "  Est. tokens  :    486,844  (4 chars / token heuristic)\n",
      "\n",
      "  Model                                   Limit    % used   Fits?\n",
      "  ----------------------------------------------------------\n",
      "  GPT-4o / GPT-4o-mini                  128,000      380%   ✗\n",
      "  Claude Sonnet 4.6 (200 k)             200,000      243%   ✗\n"
     ]
    }
   ],
   "source": [
    "raw_text   = HTML_FILE.read_text(encoding=\"utf-8\")\n",
    "raw_bytes  = HTML_FILE.stat().st_size\n",
    "raw_chars  = len(raw_text)\n",
    "raw_lines  = raw_text.count(\"\\n\")\n",
    "raw_tokens = raw_chars // 4\n",
    "\n",
    "print(\"=\" * 56)\n",
    "print(\"  RAW HTML FILE METRICS\")\n",
    "print(\"=\" * 56)\n",
    "print(f\"  File size    : {raw_bytes:>10,} bytes  ({raw_bytes/1024/1024:.2f} MB)\")\n",
    "print(f\"  Characters   : {raw_chars:>10,}\")\n",
    "print(f\"  Lines        : {raw_lines:>10,}\")\n",
    "print(f\"  Est. tokens  : {raw_tokens:>10,}  (4 chars / token heuristic)\")\n",
    "print()\n",
    "\n",
    "windows = [\n",
    "    (\"GPT-4o / GPT-4o-mini\",       128_000),\n",
    "    (\"Claude Sonnet 4.6 (200 k)\",  200_000),\n",
    "]\n",
    "print(f\"  {'Model':<36} {'Limit':>8}   {'% used':>7}   Fits?\")\n",
    "print(f\"  {'-'*58}\")\n",
    "for name, limit in windows:\n",
    "    pct  = raw_tokens / limit * 100\n",
    "    icon = \"✓\" if raw_tokens <= limit else \"✗\"\n",
    "    print(f\"  {name:<36} {limit:>8,}   {pct:>6.0f}%   {icon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Element type                            Chars   ~Tokens  % of file\n",
      "  --------------------------------------------------------------------\n",
      "  <div> / <section> wrappers         1,914,066.0  478,516.5      98.3%  ███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████\n",
      "  <a> link tags                        56,954.0  14,238.5       2.9%  ██████████████\n",
      "  <img> tags                           17,510.9   4,377.7       0.9%  ████\n",
      "  <style> blocks  (inline CSS)          9,639.9   2,409.9       0.5%  ██\n",
      "  <script> blocks (JS / JSON-LD)        9,466.9   2,366.7       0.5%  ██\n",
      "  <svg> elements                           33.0       8.2       0.0%  \n",
      "  Text nodes / other                        0.0       0.0       0.0%  \n",
      "  --------------------------------------------------------------------\n",
      "  TOTAL                               1,947,379   486,844     100.0%\n",
      "\n",
      "  98%+ comes from <div>/<section> wrappers — layout noise, not content.\n",
      "  The extractors discard all of this and emit only accessibility-relevant data.\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(raw_text, \"lxml\")\n",
    "\n",
    "def char_size(tags):\n",
    "    return sum(len(str(el)) for el in tags)\n",
    "\n",
    "sections = {\n",
    "    \"<style> blocks  (inline CSS)\":    char_size(soup.find_all(\"style\")),\n",
    "    \"<script> blocks (JS / JSON-LD)\":  char_size(soup.find_all(\"script\")),\n",
    "    \"<svg> elements\":                  char_size(soup.find_all(\"svg\")),\n",
    "    \"<img> tags\":                      char_size(soup.find_all(\"img\")),\n",
    "    \"<a> link tags\":                   char_size(soup.find_all(\"a\")),\n",
    "    \"<div> / <section> wrappers\":      char_size(soup.find_all([\"div\", \"section\"])),\n",
    "}\n",
    "accounted = sum(sections.values())\n",
    "sections[\"Text nodes / other\"] = max(0, raw_chars - accounted)\n",
    "\n",
    "print(f\"  {'Element type':<34} {'Chars':>10}  {'~Tokens':>8}  {'% of file':>9}\")\n",
    "print(\"  \" + \"-\" * 68)\n",
    "for label, chars in sorted(sections.items(), key=lambda x: -x[1]):\n",
    "    toks = chars // 4\n",
    "    pct  = chars / raw_chars * 100\n",
    "    bar  = \"█\" * int(pct / 2)\n",
    "    print(f\"  {label:<34} {chars/10:>10,}  {toks/10:>8,}  {pct/10:>8.1f}%  {bar}\")\n",
    "print(\"  \" + \"-\" * 68)\n",
    "print(f\"  {'TOTAL':<34} {raw_chars:>10,}  {raw_chars//4:>8,}  {'100.0%':>9}\")\n",
    "print()\n",
    "print(\"  98%+ comes from <div>/<section> wrappers — layout noise, not content.\")\n",
    "print(\"  The extractors discard all of this and emit only accessibility-relevant data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-005",
   "metadata": {},
   "source": [
    "---\n",
    "## Pass 1 — Programmatic Binary Checks\n",
    "\n",
    "**Run first.** These checks are objectively verifiable directly from the HTML — no LLM\n",
    "interpretation needed. Pass 1 produces two outputs:\n",
    "\n",
    "1. **Direct findings** — binary violations reported immediately\n",
    "2. **Filter sets** — used in the next step to exclude already-caught items from Pass 2\n",
    "\n",
    "### Filtering rules\n",
    "\n",
    "When a rule fires, it means something is missing or broken. There is nothing\n",
    "for the LLM to evaluate quality of — so those items are excluded from Pass 2.\n",
    "\n",
    "| Rule that fires | Excluded from Pass 2 |\n",
    "|---|---|\n",
    "| `PAGE_TITLE_001` / `_003` (missing/empty title) | CL01 Prompt 1 skipped entirely |\n",
    "| `IFRAME_001` / `_002` (missing/empty iframe title) | Those iframes filtered from CL01 Prompt 5 |\n",
    "| `HEAD_004` (empty heading) | That heading filtered from CL01 Prompt 2 |\n",
    "| `FORM_LABEL_001` (no label at all) | That field filtered from CL02 Prompt 1 |\n",
    "| `FORM_INSTR_001` (broken `aria-describedby`) | That field filtered from CL02 Prompt 5 |\n",
    "| `NON_TEXT_001` (missing `alt` on `<img>`) | That image filtered from CL03 Prompts 1 & 2 |\n",
    "| `NON_TEXT_002` (actionable image, no/empty alt) | That image filtered from CL03 Prompt 3 |\n",
    "\n",
    "`FORM_LABEL_003` (placeholder-only), `HEAD_001` (skipped level), and `FORM_GROUP_001` (no legend)\n",
    "are **not** filtered — the LLM adds value by evaluating severity and suggesting remediation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "  PASS 1 — PROGRAMMATIC AUDIT RESULTS  (home.html)\n",
      "==============================================================\n",
      "\n",
      "  CL01 Semantic  (29 rules checked, 1233 issue(s) found)\n",
      "  ──────────────────────────────────────────────\n",
      "    [HEAD_001] Skipped heading level  × 5\n",
      "    [HEAD_002] Multiple <h1> elements  × 5\n",
      "    [LAND_002] Multiple main landmarks  × 5\n",
      "    [LAND_004] Multiple contentinfo landmarks  × 5\n",
      "    [LAND_005] Multiple 'navigation' landmarks without accessible labels  × 39\n",
      "    [LAND_006] Content outside landmark regions\n",
      "    [LINK_001] Link without accessible name  × 197\n",
      "    [NAV_003] Skip link is not first focusable element\n",
      "    [PAGE_TITLE_002] Multiple <title> elements  × 3\n",
      "    [PARSE_001] Duplicate ID  × 972\n",
      "\n",
      "  CL02 Forms  (7 rules checked, 0 issue(s) found)\n",
      "  ──────────────────────────────────────────────\n",
      "    No issues found.\n",
      "\n",
      "  CL03 Non-text  (7 rules checked, 124 issue(s) found)\n",
      "  ──────────────────────────────────────────────\n",
      "    [NON_TEXT_002] Actionable image missing alt text  × 124\n",
      "\n",
      "  ────────────────────────────────────────────────────────\n",
      "  Total Pass 1 issues : 1357\n",
      "  These are definitive findings — no LLM interpretation needed.\n"
     ]
    }
   ],
   "source": [
    "# ── Load and run all three programmatic checkers ──────────────────────────────\n",
    "prog_sem   = load_module(\"semantic_checklist_01\", PROG_DIR)\n",
    "prog_forms = load_module(\"forms_checklist_02\",    PROG_DIR)\n",
    "prog_ntext = load_module(\"nontext_checklist_03\",  PROG_DIR)\n",
    "\n",
    "sem_issues   = prog_sem.audit_html_file(str(HTML_FILE))\n",
    "form_issues  = prog_forms.audit_forms(str(HTML_FILE))\n",
    "ntext_issues = prog_ntext.audit_nontext(str(HTML_FILE))\n",
    "\n",
    "all_prog_issues = sem_issues + form_issues + ntext_issues\n",
    "\n",
    "# ── Display results ────────────────────────────────────────────────────────────\n",
    "checkers = [\n",
    "    (\"CL01 Semantic\",  sem_issues,   29),\n",
    "    (\"CL02 Forms\",     form_issues,   7),\n",
    "    (\"CL03 Non-text\",  ntext_issues,  7),\n",
    "]\n",
    "\n",
    "print(\"=\" * 62)\n",
    "print(\"  PASS 1 — PROGRAMMATIC AUDIT RESULTS  (home.html)\")\n",
    "print(\"=\" * 62)\n",
    "\n",
    "total = 0\n",
    "for checker_name, issues, rule_count in checkers:\n",
    "    print(f\"\\n  {checker_name}  ({rule_count} rules checked, {len(issues)} issue(s) found)\")\n",
    "    print(f\"  {'─'*46}\")\n",
    "    if not issues:\n",
    "        print(\"    No issues found.\")\n",
    "    else:\n",
    "        by_rule = {}\n",
    "        for i in issues:\n",
    "            key = i[\"rule_id\"]\n",
    "            if key not in by_rule:\n",
    "                by_rule[key] = (i[\"rule_name\"], 0)\n",
    "            by_rule[key] = (by_rule[key][0], by_rule[key][1] + 1)\n",
    "        for rule_id in sorted(by_rule):\n",
    "            rule_name, count = by_rule[rule_id]\n",
    "            suffix = f\"  × {count}\" if count > 1 else \"\"\n",
    "            print(f\"    [{rule_id}] {rule_name}{suffix}\")\n",
    "    total += len(issues)\n",
    "\n",
    "print(f\"\\n  {'─'*56}\")\n",
    "print(f\"  Total Pass 1 issues : {total}\")\n",
    "print(f\"  These are definitive findings — no LLM interpretation needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1 → Pass 2 filter summary\n",
      "  Filter                                              Active?\n",
      "  ────────────────────────────────────────────────────────────\n",
      "  Skip CL01 Prompt 1 (page title)                          no\n",
      "  Filter iframes from CL01 Prompt 5                        no\n",
      "  Filter empty headings from CL01 Prompt 2                 no\n",
      "  Filter unlabelled fields from CL02 Prompt 1              no\n",
      "  Filter broken-desc fields from CL02 Prompt 5             no\n",
      "  Filter no-alt images from CL03 Prompts 1 & 2             no\n",
      "  Filter no-alt actionable images from CL03 Prompt 3      YES\n",
      "\n",
      "  Items marked 'no' pass through to LLM quality evaluation unchanged.\n"
     ]
    }
   ],
   "source": [
    "# ── Build filter sets for Pass 2 ──────────────────────────────────────────────\n",
    "# Filtering is applied directly on extracted payload values, not by ID matching,\n",
    "# since many elements lack IDs. Each filter is a simple lambda over the payload.\n",
    "\n",
    "skip_title_prompt = any(\n",
    "    i[\"rule_id\"] in (\"PAGE_TITLE_001\", \"PAGE_TITLE_003\")\n",
    "    for i in sem_issues\n",
    ")\n",
    "\n",
    "iframe_rules_fired = {i[\"rule_id\"] for i in sem_issues\n",
    "                      if i[\"rule_id\"] in (\"IFRAME_001\", \"IFRAME_002\")}\n",
    "\n",
    "head_empty_fired   = any(i[\"rule_id\"] == \"HEAD_004\" for i in sem_issues)\n",
    "form_label_fired   = any(i[\"rule_id\"] == \"FORM_LABEL_001\" for i in form_issues)\n",
    "form_instr_fired   = any(i[\"rule_id\"] == \"FORM_INSTR_001\" for i in form_issues)\n",
    "ntext_missing_fired = any(i[\"rule_id\"] == \"NON_TEXT_001\" for i in ntext_issues)\n",
    "ntext_action_fired  = any(i[\"rule_id\"] == \"NON_TEXT_002\" for i in ntext_issues)\n",
    "\n",
    "print(\"Pass 1 → Pass 2 filter summary\")\n",
    "print(f\"  {'Filter':<50} {'Active?':>8}\")\n",
    "print(f\"  {'─'*60}\")\n",
    "print(f\"  {'Skip CL01 Prompt 1 (page title)':<50} {'YES' if skip_title_prompt else 'no':>8}\")\n",
    "print(f\"  {'Filter iframes from CL01 Prompt 5':<50} {'YES' if iframe_rules_fired else 'no':>8}\")\n",
    "print(f\"  {'Filter empty headings from CL01 Prompt 2':<50} {'YES' if head_empty_fired else 'no':>8}\")\n",
    "print(f\"  {'Filter unlabelled fields from CL02 Prompt 1':<50} {'YES' if form_label_fired else 'no':>8}\")\n",
    "print(f\"  {'Filter broken-desc fields from CL02 Prompt 5':<50} {'YES' if form_instr_fired else 'no':>8}\")\n",
    "print(f\"  {'Filter no-alt images from CL03 Prompts 1 & 2':<50} {'YES' if ntext_missing_fired else 'no':>8}\")\n",
    "print(f\"  {'Filter no-alt actionable images from CL03 Prompt 3':<50} {'YES' if ntext_action_fired else 'no':>8}\")\n",
    "print()\n",
    "print(\"  Items marked 'no' pass through to LLM quality evaluation unchanged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-008",
   "metadata": {},
   "source": [
    "---\n",
    "## Pass 2 — LLM Semantic Quality Evaluation\n",
    "\n",
    "With Pass 1 complete, we now extract structured JSON payloads for the LLM.\n",
    "Each extractor pulls only accessibility-relevant content — discarding all CSS,\n",
    "JavaScript, layout divs, and inline styles. The filter sets built above are then\n",
    "applied before computing prompt slices.\n",
    "\n",
    "The LLM evaluates what programmatic tools cannot: whether present attributes are\n",
    "*meaningful*, *accurate*, and *sufficient* for screen reader users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-009",
   "metadata": {},
   "source": [
    "---\n",
    "### Checklist 01 — Semantic Structure & Navigation\n",
    "\n",
    "**Extractor**: `llm_preprocessing/semantic_checklist_01.py`  \n",
    "**Prompts**: `llm/semantic_checklist_01.txt` (up to 7)\n",
    "\n",
    "| Section | WCAG | What the LLM judges |\n",
    "|---|---|---|\n",
    "| `page_title` + `headings` | 2.4.2, 1.3.1 | Are title and headings meaningful and logically structured? |\n",
    "| `flagged_links` | 2.4.4 | Are short/generic link texts clear when read out of context? |\n",
    "| `landmarks` | 1.3.6 | Are multiple navs differentiated? Is the structure appropriate? |\n",
    "| `tables` / `iframes` | 1.3.1, 4.1.2 | Caption/header clarity; iframe title meaningfulness |\n",
    "\n",
    "*Binary checks (missing title, invalid lang, duplicate IDs, missing iframe title) are already\n",
    "covered by Pass 1 — `language` is included in the payload for context only.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist 01 — Extraction summary\n",
      "  Section                  Items\n",
      "  ────────────────────────────────\n",
      "  language                             'en-US'\n",
      "  page_title                   —\n",
      "  headings                   280\n",
      "  images                     306  (missing=0, empty=134, has_alt=172)\n",
      "  flagged_links               76\n",
      "  forms                       16\n",
      "  buttons                     15\n",
      "  landmarks                   62\n",
      "  tables                       0\n",
      "  iframes                     15\n",
      "\n",
      "  Raw payload :   70,644 chars  |  ~17,661 tokens\n"
     ]
    }
   ],
   "source": [
    "cl01     = load_module(\"semantic_checklist_01\", PREPROCESSING)\n",
    "cl01_pay = cl01.extract(str(HTML_FILE))\n",
    "cl01_json = json.dumps(cl01_pay, indent=2)\n",
    "cl01_tok  = estimate_tokens(cl01_json)\n",
    "\n",
    "print(\"Checklist 01 — Extraction summary\")\n",
    "print(f\"  {'Section':<22}  {'Items':>6}\")\n",
    "print(f\"  {'─'*32}\")\n",
    "for key, val in cl01_pay.items():\n",
    "    if isinstance(val, list):\n",
    "        print(f\"  {key:<22}  {len(val):>6}\")\n",
    "    elif key == \"images\":\n",
    "        total = sum(len(v) for v in val.values())\n",
    "        miss  = len(val[\"missing_alt\"])\n",
    "        empty = len(val[\"empty_alt\"])\n",
    "        has   = len(val[\"has_alt\"])\n",
    "        print(f\"  {key:<22}  {total:>6}  (missing={miss}, empty={empty}, has_alt={has})\")\n",
    "    elif isinstance(val, dict):\n",
    "        print(f\"  {key:<22}  {'—':>6}\")\n",
    "    else:\n",
    "        print(f\"  {key:<22}  {repr(val)[:20]:>20}\")\n",
    "print(f\"\\n  Raw payload : {len(cl01_json):>8,} chars  |  ~{cl01_tok:>6,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-011",
   "metadata": {},
   "outputs": [],
   "source": "# ── CL01 prompt slices — with Pass 1 filters applied ─────────────────────────\n\n# Filter: empty headings already caught by HEAD_004\nheadings_filtered = [h for h in cl01_pay[\"headings\"] if h.get(\"text\", \"\").strip()]\n\n# Filter: iframes with missing/empty titles already caught by IFRAME_001/002\niframes_filtered  = [f for f in cl01_pay[\"iframes\"]\n                     if f.get(\"title\") and str(f[\"title\"]).strip()]\n\ncl01_slices = {\n    \"1. Page Title\":         None if skip_title_prompt\n                             else {\"page_title\": cl01_pay[\"page_title\"]},\n    \"2. Heading Structure\":  {\"page_title\": cl01_pay[\"page_title\"],\n                              \"headings\":   headings_filtered},\n    \"3. Link Clarity\":       {\"flagged_links\": cl01_pay[\"flagged_links\"]},\n    \"4. Table Semantics\":    {\"tables\":    cl01_pay[\"tables\"]},\n    \"5. Iframe Titles\":      {\"iframes\":   iframes_filtered},\n    \"6. Landmark Structure\": {\"landmarks\": cl01_pay[\"landmarks\"]},\n    \"7. Combined Summary\":   None,  # skipped — redundant with individual prompts\n}\n\nprint(\"Checklist 01 — Token budget per prompt (after Pass 1 filtering)\")\nprint(f\"  {'Prompt':<26}  {'Items':>6}  {'~Tokens':>8}  Note\")\nprint(f\"  {'─'*60}\")\nitem_counts = {\n    \"1. Page Title\":         0 if skip_title_prompt else 1,\n    \"2. Heading Structure\":  len(headings_filtered),\n    \"3. Link Clarity\":       len(cl01_pay[\"flagged_links\"]),\n    \"4. Table Semantics\":    len(cl01_pay[\"tables\"]),\n    \"5. Iframe Titles\":      len(iframes_filtered),\n    \"6. Landmark Structure\": len(cl01_pay[\"landmarks\"]),\n    \"7. Combined Summary\":   0,\n}\nfor name, data in cl01_slices.items():\n    if data is None:\n        note = \"SKIPPED (Pass 1: missing/empty title)\" if \"Title\" in name else \"SKIPPED (summary)\"\n        print(f\"  {name:<26}  {'—':>6}  {'—':>8}  {note}\")\n    else:\n        t = estimate_tokens(json.dumps(data, indent=2))\n        n = item_counts[name]\n        orig_n = {\n            \"2. Heading Structure\":  len(cl01_pay[\"headings\"]),\n            \"5. Iframe Titles\":      len(cl01_pay[\"iframes\"]),\n        }.get(name)\n        note = f\"  (filtered from {orig_n})\" if orig_n and orig_n != n else \"\"\n        print(f\"  {name:<26}  {n:>6}  {t:>8,}{note}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-012",
   "metadata": {},
   "source": [
    "---\n",
    "### Checklist 02 — Form Labels, Instructions & Groups\n",
    "\n",
    "**Extractor**: `llm_preprocessing/forms_checklist_02.py`  \n",
    "**Prompts**: `llm/forms_checklist_02.txt` (up to 6)\n",
    "\n",
    "| Section | WCAG | What the LLM judges |\n",
    "|---|---|---|\n",
    "| `fields.effective_label` | 1.3.1, 2.4.6 | Is the label descriptive and meaningful? |\n",
    "| `fields.label_source` | 1.3.1 | Is `placeholder_only` being used as a label? (severity + remediation) |\n",
    "| `fields.instructions` | 3.3.2 | Are aria-describedby instructions clear and helpful? |\n",
    "| `fields.required` | 3.3.2 | Is required status communicated in the label, not just via attribute? |\n",
    "| `groups.legend` | 1.3.1 | Does the legend provide sufficient group context? |\n",
    "\n",
    "*Fields with no label at all (`FORM_LABEL_001`) are filtered from Prompt 1 — there is\n",
    "no label quality to assess. Fields with broken `aria-describedby` (`FORM_INSTR_001`)\n",
    "are filtered from Prompt 5 — the instructions are unreachable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist 02 — Forms summary\n",
      "  Total forms            : 16\n",
      "  Total fields           : 16\n",
      "  Total groups           : 0\n",
      "  Required fields        : 5\n",
      "  Fields with aria-desc  : 0\n",
      "  Orphan labels          : 0\n",
      "\n",
      "  Label source breakdown:\n",
      "    label_for                16\n",
      "\n",
      "  Raw payload :    9,975 chars  |  ~ 2,493 tokens\n"
     ]
    }
   ],
   "source": [
    "cl02      = load_module(\"forms_checklist_02\", PREPROCESSING)\n",
    "cl02_pay  = cl02.extract(str(HTML_FILE))\n",
    "cl02_json = json.dumps(cl02_pay, indent=2)\n",
    "cl02_tok  = estimate_tokens(cl02_json)\n",
    "\n",
    "forms      = cl02_pay[\"forms\"]\n",
    "all_fields = [f for form in forms for f in form[\"fields\"]]\n",
    "all_groups = [g for form in forms for g in form[\"groups\"]]\n",
    "\n",
    "src_counts = Counter(f[\"label_source\"] for f in all_fields)\n",
    "required   = [f for f in all_fields if f[\"required\"]]\n",
    "with_instr = [f for f in all_fields if f[\"instructions\"]]\n",
    "\n",
    "print(\"Checklist 02 — Forms summary\")\n",
    "print(f\"  Total forms            : {len(forms)}\")\n",
    "print(f\"  Total fields           : {len(all_fields)}\")\n",
    "print(f\"  Total groups           : {len(all_groups)}\")\n",
    "print(f\"  Required fields        : {len(required)}\")\n",
    "print(f\"  Fields with aria-desc  : {len(with_instr)}\")\n",
    "print(f\"  Orphan labels          : {len(cl02_pay['orphan_labels'])}\")\n",
    "print()\n",
    "print(\"  Label source breakdown:\")\n",
    "for src, count in src_counts.most_common():\n",
    "    flag = \"  ⚠\" if src in (\"placeholder_only\", \"none\") else \"\"\n",
    "    print(f\"    {src:<22} {count:>4}{flag}\")\n",
    "print()\n",
    "print(f\"  Raw payload : {len(cl02_json):>8,} chars  |  ~{cl02_tok:>6,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-014",
   "metadata": {},
   "outputs": [],
   "source": "# ── CL02 prompt slices — with Pass 1 filters applied ─────────────────────────\n\n# Filter: fields with no label (FORM_LABEL_001) excluded from label quality prompt\nfields_with_labels = [f for f in all_fields if f[\"label_source\"] != \"none\"]\n\n# Filter: fields with broken aria-describedby (FORM_INSTR_001) excluded from instructions prompt\nfields_with_valid_instr = [f for f in all_fields if f[\"instructions\"]]\n\nplaceholder_only = [f for f in all_fields if f[\"label_source\"] == \"placeholder_only\"]\n\ncl02_slices = {\n    \"1. Label Quality\":        {\"fields\": fields_with_labels},\n    \"2. Placeholder-as-Label\": {\"fields\": placeholder_only},\n    \"3. Group Label Quality\":  {\"groups\": all_groups},\n    \"4. Required Indicators\":  {\"fields\": required},\n    \"5. Instructions Quality\": {\"fields\": fields_with_valid_instr},\n    \"6. Overall Form Summary\": None,  # skipped — redundant with individual prompts\n}\n\nprint(\"Checklist 02 — Token budget per prompt (after Pass 1 filtering)\")\nprint(f\"  {'Prompt':<28}  {'Items':>6}  {'~Tokens':>8}  Note\")\nprint(f\"  {'─'*62}\")\ncounts = {\n    \"1. Label Quality\":        len(fields_with_labels),\n    \"2. Placeholder-as-Label\": len(placeholder_only),\n    \"3. Group Label Quality\":  len(all_groups),\n    \"4. Required Indicators\":  len(required),\n    \"5. Instructions Quality\": len(fields_with_valid_instr),\n    \"6. Overall Form Summary\": 0,\n}\norig_counts = {\n    \"1. Label Quality\": len(all_fields),\n}\nfor name, data in cl02_slices.items():\n    if data is None:\n        print(f\"  {name:<28}  {'—':>6}  {'—':>8}  SKIPPED (summary)\")\n        continue\n    n = counts[name]\n    t = estimate_tokens(json.dumps(data, indent=2))\n    orig = orig_counts.get(name)\n    note = f\"  (filtered from {orig})\" if orig and orig != n else \"\"\n    if n == 0:\n        note = \"  SKIPPED (no items)\"\n    print(f\"  {name:<28}  {n:>6}  {t:>8,}{note}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-015",
   "metadata": {},
   "source": [
    "---\n",
    "### Checklist 03 — Non-text Content: Images, SVG, Icons & Media\n",
    "\n",
    "**Extractor**: `llm_preprocessing/nontext_checklist_03.py`  \n",
    "**Prompts**: `llm/nontext_checklist_03.txt` (up to 8)\n",
    "\n",
    "Images are split into four categories because the evaluation criteria differ:\n",
    "\n",
    "| Category | Pass 1 interaction | LLM evaluates |\n",
    "|---|---|---|\n",
    "| `informative` | Images with `alt=None` filtered out (NON_TEXT_001) | Is the alt text accurate and concise? |\n",
    "| `decorative` | Images with `alt=None` filtered out (NON_TEXT_001) | Is empty alt genuinely correct — or does the image convey content? |\n",
    "| `actionable` | Images with empty/null alt filtered out (NON_TEXT_002) | Does alt describe the destination/action, not the image appearance? |\n",
    "| `complex` | No filtering — complex images are identified by content hints | Is the long description adequate for charts/diagrams? |\n",
    "\n",
    "`sole_content` on icon fonts: when `True` + `aria-hidden=True` → critical unlabeled control (LLM flags severity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist 03 — Non-text content summary\n",
      "  Images — informative   : 60\n",
      "  Images — decorative    : 10\n",
      "  Images — actionable    : 236  (in <a> or <button>)\n",
      "  Images — complex       : 0\n",
      "  Total images           : 306\n",
      "  SVGs (non-hidden)      : 5\n",
      "  Icon fonts (unique)    : 12\n",
      "  Media elements         : 0\n",
      "\n",
      "  Pre-detected quality flags:\n",
      "    Informative images with alt_flags  : 0\n",
      "    Icons: sole_content + aria-hidden  : 1  ← unlabeled controls\n",
      "    SVGs with no title or aria-label   : 5\n",
      "\n",
      "  Raw payload :   76,663 chars  |  ~19,165 tokens\n"
     ]
    }
   ],
   "source": [
    "cl03      = load_module(\"nontext_checklist_03\", PREPROCESSING)\n",
    "cl03_pay  = cl03.extract(str(HTML_FILE))\n",
    "cl03_json = json.dumps(cl03_pay, indent=2)\n",
    "cl03_tok  = estimate_tokens(cl03_json)\n",
    "\n",
    "img = cl03_pay[\"images\"]\n",
    "total_images = sum(len(v) for v in img.values())\n",
    "\n",
    "print(\"Checklist 03 — Non-text content summary\")\n",
    "print(f\"  Images — informative   : {len(img['informative'])}\")\n",
    "print(f\"  Images — decorative    : {len(img['decorative'])}\")\n",
    "print(f\"  Images — actionable    : {len(img['actionable'])}  (in <a> or <button>)\")\n",
    "print(f\"  Images — complex       : {len(img['complex'])}\")\n",
    "print(f\"  Total images           : {total_images}\")\n",
    "print(f\"  SVGs (non-hidden)      : {len(cl03_pay['svgs'])}\")\n",
    "print(f\"  Icon fonts (unique)    : {len(cl03_pay['icon_fonts'])}\")\n",
    "print(f\"  Media elements         : {len(cl03_pay['media'])}\")\n",
    "print()\n",
    "\n",
    "flagged_imgs     = [i for i in img[\"informative\"] if i[\"alt_flags\"]]\n",
    "unlabeled_icons  = [ic for ic in cl03_pay[\"icon_fonts\"]\n",
    "                    if ic[\"sole_content\"] and ic[\"aria_hidden\"] and not ic[\"aria_label\"]]\n",
    "broken_svgs      = [s for s in cl03_pay[\"svgs\"] if not s[\"title\"] and not s[\"aria_label\"]]\n",
    "\n",
    "print(\"  Pre-detected quality flags:\")\n",
    "print(f\"    Informative images with alt_flags  : {len(flagged_imgs)}\")\n",
    "print(f\"    Icons: sole_content + aria-hidden  : {len(unlabeled_icons)}  ← unlabeled controls\")\n",
    "print(f\"    SVGs with no title or aria-label   : {len(broken_svgs)}\")\n",
    "print()\n",
    "print(f\"  Raw payload : {len(cl03_json):>8,} chars  |  ~{cl03_tok:>6,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-017",
   "metadata": {},
   "outputs": [],
   "source": "# ── CL03 prompt slices — with Pass 1 filters applied ─────────────────────────\n\n# Filter: images with no alt at all (NON_TEXT_001) excluded from informative + decorative prompts\n# In the extractor, images with alt=None don't appear in informative/decorative (they're\n# in cl01_pay[\"images\"][\"missing_alt\"]), so no further filtering needed for Prompts 1 & 2.\n\n# Filter: actionable images with no/empty alt (NON_TEXT_002) excluded from Prompt 3\nactionable_filtered = [\n    im for im in img[\"actionable\"]\n    if im.get(\"alt\") and str(im[\"alt\"]).strip() and im[\"alt\"] != \"None\"\n]\n\ncl03_slices = {\n    \"1. Informative Alt Quality\":  {\"images\": img[\"informative\"]},\n    \"2. Decorative Verification\":  {\"images\": img[\"decorative\"]},\n    \"3. Actionable Image Alt\":     {\"images\": actionable_filtered},\n    \"4. Complex Descriptions\":     {\"images\": img[\"complex\"]},\n    \"5. SVG Accessibility\":        {\"svgs\":   cl03_pay[\"svgs\"]},\n    \"6. Icon Font Accessibility\":  {\"icon_fonts\": cl03_pay[\"icon_fonts\"]},\n    \"7. Media Captions\":           {\"media\":  cl03_pay[\"media\"]},\n    \"8. Overall Summary\":          None,  # skipped — redundant with individual prompts\n}\n\nprint(\"Checklist 03 — Token budget per prompt (after Pass 1 filtering)\")\nprint(f\"  {'Prompt':<30}  {'Items':>6}  {'~Tokens':>8}  Note\")\nprint(f\"  {'─'*64}\")\norig_actionable = len(img[\"actionable\"])\nfor name, data in cl03_slices.items():\n    if data is None:\n        print(f\"  {name:<30}  {'—':>6}  {'—':>8}  SKIPPED (summary)\")\n        continue\n    items = list(data.values())[0] if len(data) == 1 else []\n    n = len(items) if isinstance(items, list) else 0\n    t = estimate_tokens(json.dumps(data, indent=2))\n    note = \"\"\n    if name == \"3. Actionable Image Alt\" and orig_actionable != n:\n        note = f\"  (filtered from {orig_actionable}: {orig_actionable - n} had no/empty alt)\"\n    if n == 0:\n        note = \"  SKIPPED (no items)\"\n    print(f\"  {name:<30}  {n:>6}  {t:>8,}{note}\")"
  },
  {
   "cell_type": "markdown",
   "id": "jyopflg864a",
   "metadata": {},
   "source": [
    "---\n",
    "## Pass 2 — Claude API Calls\n",
    "\n",
    "With the filtered payload slices ready, we now send each slice to the Claude API.\n",
    "\n",
    "The `llm_client` module (`processing_scripts/llm_client/`) handles:\n",
    "- Loading numbered prompt templates from `llm/*.txt`\n",
    "- Filling the `{payload}` placeholder with serialised JSON\n",
    "- Calling the Anthropic Messages API\n",
    "- Stripping markdown code fences from responses\n",
    "- Aggregating token usage across all calls\n",
    "\n",
    "Configure **MODEL** and **TEMPERATURE** below to compare models or prompt strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hl5ld92k01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── API Configuration ─────────────────────────────────────────────────────────\n",
    "# Adjust these to compare models or prompt strategies\n",
    "\n",
    "MODEL       = \"claude-haiku-4-5-20251001\"   # e.g. \"claude-haiku-4-5-20251001\"\n",
    "TEMPERATURE = 0.1                   # Low temperature for consistent JSON output\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Model       : {MODEL}\")\n",
    "print(f\"Temperature : {TEMPERATURE}\")\n",
    "print(f\"Reports dir : {REPORTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yzqqj0gdpn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Set up LLM client ─────────────────────────────────────────────────────────\n",
    "sys.path.insert(0, str(NOTEBOOK_DIR))\n",
    "from llm_client import AuditClient, load_all_prompts, run_all as _run_all\n",
    "\n",
    "all_prompts = load_all_prompts(PROMPTS_DIR)\n",
    "client      = AuditClient(model=MODEL, temperature=TEMPERATURE)\n",
    "\n",
    "for stem, prompts in sorted(all_prompts.items()):\n",
    "    print(f\"  {stem}: {len(prompts)} prompts loaded\")\n",
    "print(f\"\\nClient ready — {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bre8mq0nv1l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run Pass 2 — all checklist prompts ───────────────────────────────────────\n",
    "# ⚠️  This makes real API calls and incurs token costs.\n",
    "\n",
    "all_slices = {\n",
    "    \"semantic_checklist_01\": cl01_slices,\n",
    "    \"forms_checklist_02\":    cl02_slices,\n",
    "    \"nontext_checklist_03\":  cl03_slices,\n",
    "}\n",
    "\n",
    "api_report = _run_all(client, all_prompts, all_slices, verbose=True)\n",
    "\n",
    "print(f\"\\n{'─'*50}\")\n",
    "print(f\"  Total input tokens : {api_report['total_usage']['input_tokens']:,}\")\n",
    "print(f\"  Total output tokens: {api_report['total_usage']['output_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mgdetq5ffw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save structured report ────────────────────────────────────────────────────\n",
    "import datetime\n",
    "\n",
    "timestamp   = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_slug  = MODEL.replace(\"/\", \"-\")\n",
    "report_path = REPORTS_DIR / f\"audit_{timestamp}_{model_slug}.json\"\n",
    "\n",
    "full_report = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\":    timestamp,\n",
    "        \"model\":        MODEL,\n",
    "        \"temperature\":  TEMPERATURE,\n",
    "        \"html_file\":    str(HTML_FILE),\n",
    "        \"html_bytes\":   raw_bytes,\n",
    "        \"raw_tokens\":   raw_tokens,\n",
    "    },\n",
    "    \"pass_1_programmatic\": {\n",
    "        \"issues\": all_prog_issues,\n",
    "        \"total\":  len(all_prog_issues),\n",
    "    },\n",
    "    \"pass_2_llm\": api_report,\n",
    "}\n",
    "\n",
    "report_path.write_text(json.dumps(full_report, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Report saved → {report_path}\")\n",
    "print(f\"File size    : {report_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-018",
   "metadata": {},
   "source": [
    "---\n",
    "## Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Token reduction summary ───────────────────────────────────────────────────\n",
    "cl01_json = json.dumps(cl01_pay, indent=2)\n",
    "cl02_json = json.dumps(cl02_pay, indent=2)\n",
    "cl03_json = json.dumps(cl03_pay, indent=2)\n",
    "\n",
    "checklists_tok = {\n",
    "    \"CL01 Semantic\":  cl01_json,\n",
    "    \"CL02 Forms\":     cl02_json,\n",
    "    \"CL03 Non-text\":  cl03_json,\n",
    "}\n",
    "combined_tok = sum(estimate_tokens(j) for j in checklists_tok.values())\n",
    "\n",
    "print(\"=\" * 64)\n",
    "print(\"  TOKEN REDUCTION — PASS 2 PAYLOADS vs RAW HTML\")\n",
    "print(\"=\" * 64)\n",
    "print(f\"\\n  {'Checklist':<20}  {'Chars':>10}  {'~Tokens':>8}  {'vs raw':>8}\")\n",
    "print(f\"  {'─'*54}\")\n",
    "for name, js in checklists_tok.items():\n",
    "    tok = estimate_tokens(js)\n",
    "    pct = tok / raw_tokens * 100\n",
    "    print(f\"  {name:<20}  {len(js):>10,}  {tok:>8,}  {pct:>7.1f}%\")\n",
    "print(f\"  {'─'*54}\")\n",
    "print(f\"  {'COMBINED (all 3)':<20}  {'':>10}  {combined_tok:>8,}  \"\n",
    "      f\"{combined_tok/raw_tokens*100:>7.1f}%\")\n",
    "print(f\"  {'RAW HTML':<20}  {raw_bytes:>10,}  {raw_tokens:>8,}  {'100.0%':>8}\")\n",
    "print()\n",
    "reduction = (1 - combined_tok / raw_tokens) * 100\n",
    "print(f\"  Token reduction : {reduction:.1f}%  ({raw_tokens // combined_tok}× smaller)\")\n",
    "print(f\"  Max LLM calls   : 21  (empty slices after filtering are skipped)\")\n",
    "print(f\"  Pass 1 findings : {len(all_prog_issues)} definitive issues (no LLM needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "s8upr3uulll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "  PASS 2 — LLM RESULTS\n",
      "================================================================\n",
      "\n",
      "  semantic_checklist_01\n",
      "    Prompts run : 5  |  skipped: 1  |  errors: 1\n",
      "    Tokens      : 38,669 in / 3,309 out\n",
      "      [1. Page Title]  4 issue(s) reported\n",
      "      [2. Heading Structure]  12 issue(s) reported\n",
      "      [5. Iframe Titles]  15 issue(s) reported\n",
      "      [6. Landmark Structure]  7 issue(s) reported\n",
      "      [7. Combined Summary]  0 issue(s) reported\n",
      "    ERROR [3. Link Clarity]: Unterminated string starting at: line 442 column 5 (char 13172)\n",
      "\n",
      "  forms_checklist_02\n",
      "    Prompts run : 3  |  skipped: 3  |  errors: 0\n",
      "    Tokens      : 6,611 in / 3,419 out\n",
      "      [1. Label Quality]  16 issue(s) reported\n",
      "      [4. Required Indicators]  5 issue(s) reported\n",
      "      [6. Overall Form Summary]  0 issue(s) reported\n",
      "\n",
      "  nontext_checklist_03\n",
      "    Prompts run : 6  |  skipped: 2  |  errors: 0\n",
      "    Tokens      : 44,552 in / 10,597 out\n",
      "      [1. Informative Alt Quality]  12 issue(s) reported\n",
      "      [2. Decorative Verification]  10 issue(s) reported\n",
      "      [3. Actionable Image Alt]  19 issue(s) reported\n",
      "      [5. SVG Accessibility]  5 issue(s) reported\n",
      "      [6. Icon Font Accessibility]  12 issue(s) reported\n",
      "      [8. Overall Summary]  0 issue(s) reported\n",
      "\n",
      "  ──────────────────────────────────────────────────\n",
      "  Total API usage\n",
      "    Input tokens : 89,832\n",
      "    Output tokens: 17,325\n",
      "\n",
      "  Full report → /Users/andrew/git/visionaid-a11y-llm-audit/reports/audit_20260228_182910_claude-haiku-4-5-20251001.json\n"
     ]
    }
   ],
   "source": [
    "# ── Pass 2 results summary ────────────────────────────────────────────────────\n",
    "print(\"=\" * 64)\n",
    "print(\"  PASS 2 — LLM RESULTS\")\n",
    "print(\"=\" * 64)\n",
    "\n",
    "for stem, result in api_report[\"checklists\"].items():\n",
    "    n_done    = len(result[\"results\"])\n",
    "    n_skipped = len(result[\"skipped\"])\n",
    "    n_errors  = len(result[\"errors\"])\n",
    "    in_tok    = result[\"usage\"][\"input_tokens\"]\n",
    "    out_tok   = result[\"usage\"][\"output_tokens\"]\n",
    "\n",
    "    print(f\"\\n  {stem}\")\n",
    "    print(f\"    Prompts run : {n_done}  |  skipped: {n_skipped}  |  errors: {n_errors}\")\n",
    "    print(f\"    Tokens      : {in_tok:,} in / {out_tok:,} out\")\n",
    "\n",
    "    for label, response in result[\"results\"].items():\n",
    "        # Responses may be a dict or a list depending on the prompt schema\n",
    "        if isinstance(response, dict):\n",
    "            issues = response.get(\"issues\") or response.get(\"findings\") or []\n",
    "        elif isinstance(response, list):\n",
    "            issues = response\n",
    "        else:\n",
    "            issues = []\n",
    "        n_issues = len(issues) if isinstance(issues, list) else \"?\"\n",
    "        print(f\"      [{label}]  {n_issues} issue(s) reported\")\n",
    "\n",
    "    if result[\"errors\"]:\n",
    "        for label, err in result[\"errors\"].items():\n",
    "            print(f\"    ERROR [{label}]: {err}\")\n",
    "\n",
    "print(f\"\\n  {'─'*50}\")\n",
    "print(f\"  Total API usage\")\n",
    "print(f\"    Input tokens : {api_report['total_usage']['input_tokens']:,}\")\n",
    "print(f\"    Output tokens: {api_report['total_usage']['output_tokens']:,}\")\n",
    "print(f\"\\n  Full report → {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-020",
   "metadata": {},
   "source": [
    "---\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "home.html (1.9 MB, ~487k tokens)\n",
    "    │\n",
    "    │  PASS 1 — Programmatic (run first, milliseconds)\n",
    "    │\n",
    "    ├── programmatic/semantic_checklist_01.py   ──▶  29 rules  ──▶  binary findings\n",
    "    ├── programmatic/forms_checklist_02.py      ──▶   7 rules  ──▶  binary findings\n",
    "    └── programmatic/nontext_checklist_03.py    ──▶   7 rules  ──▶  binary findings\n",
    "                                                           │\n",
    "                                                           │  filter: items that failed a binary\n",
    "                                                           │  check excluded from Pass 2\n",
    "                                                           ▼\n",
    "    │  PASS 2 — LLM Quality Evaluation (on filtered payloads)\n",
    "    │\n",
    "    ├── llm_preprocessing/semantic_checklist_01.py + semantic_checklist_01.txt  ──▶  up to 7 LLM calls\n",
    "    ├── llm_preprocessing/forms_checklist_02.py    + forms_checklist_02.txt     ──▶  up to 6 LLM calls\n",
    "    └── llm_preprocessing/nontext_checklist_03.py  + nontext_checklist_03.txt   ──▶  up to 8 LLM calls\n",
    "                                                                                              │\n",
    "                                                                                              ▼\n",
    "                                                                                 merge all findings\n",
    "                                                                                              │\n",
    "                                                                                              ▼\n",
    "                                                                                accessibility report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-021",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "This notebook runs a complete two-pass WCAG accessibility audit:\n",
    "\n",
    "| Pass | What it does | Speed | Output |\n",
    "|------|-------------|-------|--------|\n",
    "| **Pass 1** — Programmatic | 43 binary rule checks across 3 scripts | Milliseconds | Definitive violations |\n",
    "| **Pass 2** — LLM | Up to 21 Claude API calls on filtered payloads | Seconds | Qualitative findings |\n",
    "\n",
    "### Known Issues on This Page (visionaid.org homepage)\n",
    "\n",
    "**Pass 1 — Programmatic findings** (binary, definitive):\n",
    "- Run the Pass 1 cells above to see the full list\n",
    "\n",
    "**Pass 2 — LLM evaluates:**\n",
    "- **CL01**: Heading structure, 76 flagged links, 62 landmarks, iframes (missing-title ones excluded by Pass 1)\n",
    "- **CL02**: 16 forms — label quality, placeholder detection, aria-describedby instructions\n",
    "- **CL03**: Actionable images with non-empty alt (filtered), 5 SVGs with no accessible name, icon-font buttons\n",
    "\n",
    "### Configuration\n",
    "\n",
    "To compare models or prompts, update **MODEL** and **TEMPERATURE** in the config cell and re-run from that cell down.  \n",
    "Reports are saved to `reports/audit_<timestamp>_<model>.json` — one file per run for easy comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
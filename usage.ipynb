{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Accessibility Report Generator — Usage\n",
        "\n",
        "This notebook runs the LLM accessibility audit benchmark: download the dataset, list samples, run one or more provider×prompt combinations, and view accuracy/F1.\n",
        "\n",
        "**Benchmark:** [Tabular Accessibility Dataset](https://www.mdpi.com/2306-5729/10/9/149) (Zenodo [10.5281/zenodo.17062188](https://doi.org/10.5281/zenodo.17062188))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Benchmark slices:** (1) **dynamic** — 9 samples (Angular, React, Vue, PHP). (2) **vue** — 25 delivery projects × component variants (many Vue samples). (3) **accessguru** — static HTML snippets with violations (from [AccessGuruLLM](https://github.com/NadeenAhmad/AccessGuruLLM)). Use `--slices all` or `--slices dynamic,vue,accessguru` to run all; use `--slices dynamic` for the original 9 only.\n",
        "\n",
        "**Labels:** We derive binary **has_issues** per sample; full labels are on each sample as `reference_issues`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Run the **pip install** cell below first (install once). If you get ImportError for google.genai or openai, restart the kernel and run again. Then run the path/import cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install all provider SDKs (openai, google-genai) and dotenv. Run once.\n",
        "%pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import importlib\n",
        "from pathlib import Path\n",
        "\n",
        "# Repo root: use cwd (run notebook from repo root) or parent of this file\n",
        "REPO_ROOT = Path.cwd()\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "# Reload so kernel picks up latest code after edits\n",
        "if 'src.dataset' in sys.modules:\n",
        "    importlib.reload(sys.modules['src.dataset'])\n",
        "if 'src.runner' in sys.modules:\n",
        "    importlib.reload(sys.modules['src.runner'])\n",
        "\n",
        "from src.dataset import get_benchmark_available, load_benchmark_slices\n",
        "from src.llm import get_llm\n",
        "from src.runner import run_benchmark, write_benchmark_results\n",
        "from scoring.score import score_binary, f1_binary\n",
        "\n",
        "#SLICES = (\"dynamic\", \"vue\", \"accessguru\")\n",
        "SLICES = (\"dynamic\", \"vue\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download benchmark (one-time)\n",
        "\n",
        "Run once: Zenodo (dynamic + vue) and AccessGuru (accessguru slice)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All slice data found.\n"
          ]
        }
      ],
      "source": [
        "if get_benchmark_available(slices=SLICES):\n",
        "    print(\"All slice data found.\")\n",
        "else:\n",
        "    print(\"Run: %run scripts/download_benchmark.py   and/or   %run scripts/download_accessguru.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zip already exists: /Users/andrew/git/visionaid-a11y-llm-audit/data/LLM-WebAccessibility-v2.1.0.zip\n",
            "Extracting...\n",
            "Done. Benchmark path: /Users/andrew/git/visionaid-a11y-llm-audit/data/manuandru-LLM-WebAccessibility-eae77a6/Dynamic Generated Content\n",
            "Exists: /Users/andrew/git/visionaid-a11y-llm-audit/data/accessguru/accessguru_dataset/accessguru_sampled_syntax_layout_dataset.csv\n",
            "Exists: /Users/andrew/git/visionaid-a11y-llm-audit/data/accessguru/accessguru_dataset/accessguru_sampled_semantic_violations.csv\n",
            "Exists: /Users/andrew/git/visionaid-a11y-llm-audit/data/accessguru/accessguru_dataset/Original_full_data.csv\n",
            "Done. AccessGuru slice will use CSVs in /Users/andrew/git/visionaid-a11y-llm-audit/data/accessguru/accessguru_dataset\n"
          ]
        }
      ],
      "source": [
        "%run scripts/download_benchmark.py\n",
        "%run scripts/download_accessguru.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. List samples\n",
        "\n",
        "Inspect the loaded code samples and ground-truth labels (no API calls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [dynamic] 9 samples\n",
            "  [vue] 96 samples\n",
            "Total: 105 samples.\n",
            "\n",
            "  dynamic  angular-table-accessible.js              has_issues=False  lang=js\n",
            "  dynamic  angular-table-invalid.js                 has_issues=True  lang=js\n",
            "  dynamic  php-table-accessible.php                 has_issues=False  lang=php\n",
            "  dynamic  php-table-invalid.php                    has_issues=True  lang=php\n",
            "  dynamic  react-table-accessible.js                has_issues=False  lang=js\n",
            "  dynamic  react-table-invalid.js                   has_issues=True  lang=js\n",
            "  dynamic  vue-table-accessible-llm.vue             has_issues=False  lang=vue\n",
            "  dynamic  vue-table-accessible.vue                 has_issues=False  lang=vue\n",
            "  dynamic  vue-table-invalid.vue                    has_issues=True  lang=vue\n",
            "  vue  delivery-01_accessible-composition-api   has_issues=False  lang=vue\n",
            "  vue  delivery-01_accessible-options-api       has_issues=False  lang=vue\n",
            "  vue  delivery-01_minimal-composition-api      has_issues=True  lang=vue\n",
            "  vue  delivery-01_minimal-options-api          has_issues=True  lang=vue\n",
            "  vue  delivery-02_accessible-composition-api   has_issues=False  lang=vue\n",
            "  vue  delivery-02_accessible-options-api       has_issues=False  lang=vue\n",
            "  ...\n"
          ]
        }
      ],
      "source": [
        "samples = load_benchmark_slices(slices=SLICES)\n",
        "by_slice = {}\n",
        "for s in samples:\n",
        "    by_slice.setdefault(s.slice, []).append(s)\n",
        "for sl in SLICES:\n",
        "    print(f\"  [{sl}] {len(by_slice.get(sl, []))} samples\")\n",
        "print(f\"Total: {len(samples)} samples.\\n\")\n",
        "for s in list(samples)[:15]:\n",
        "    print(f\"  {s.slice}  {s.file_name[:40]:<40} has_issues={s.has_issues}  lang={s.language}\")\n",
        "if len(samples) > 20:\n",
        "    print(\"  ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run single audit\n",
        "\n",
        "Set the API key for your provider (e.g. `GEMINI_API_KEY`, `DEEPSEEK_API_KEY`, `MOONSHOT_API_KEY`) in the environment or `.env`. Results are written to **results/** as JSON (full model responses + per-sample comparison) and TXT (summary). Token counts are printed and saved in the JSON summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running: provider=deepseek, model=deepseek-chat, prompt=audit_binary\n",
            "\n",
            "Accuracy: 49.52%\n",
            "F1 (has_issues): 66.24%\n",
            "TP=52 TN=0 FP=53 FN=0  unclear=0\n",
            "Tokens used: input=85,893  output=630\n",
            "Results written to results/ (JSON + TXT)\n"
          ]
        }
      ],
      "source": [
        "provider = \"deepseek\"   # or \"anthropic\" \"gemini\" \"deepseek\" \"kimi\"\n",
        "prompt_name = \"audit_binary\"  # or \"audit_with_reason\", \"audit_wcag_focused\"\n",
        "\n",
        "llm = get_llm(provider)\n",
        "print(f\"Running: provider={llm.provider}, model={llm.default_model}, prompt={prompt_name}\")\n",
        "results = run_benchmark(llm, prompt_name=prompt_name, slices=SLICES)\n",
        "metrics = score_binary(results)\n",
        "f1 = f1_binary(metrics)\n",
        "\n",
        "# Write solutions and comparison to results/ (JSON + TXT)\n",
        "total_in, total_out = write_benchmark_results(results, provider=provider, prompt_name=prompt_name, model=llm.default_model)\n",
        "\n",
        "print(f\"\\nAccuracy: {metrics.accuracy:.2%}\")\n",
        "print(f\"F1 (has_issues): {f1:.2%}\")\n",
        "print(f\"TP={metrics.tp} TN={metrics.tn} FP={metrics.fp} FN={metrics.fn}  unclear={metrics.unclear}\")\n",
        "print(f\"Tokens used: input={total_in:,}  output={total_out:,}\")\n",
        "print(\"Results written to results/ (JSON + TXT)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compare all provider × prompt combinations\n",
        "\n",
        "Runs OpenAI and Anthropic with each of the three prompts and prints a comparison table. Requires both API keys if you want all six runs to succeed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PROVIDERS = [\"openai\", \"anthropic\"]\n",
        "PROMPTS = [\"audit_binary\", \"audit_with_reason\", \"audit_wcag_focused\"]\n",
        "\n",
        "rows = []\n",
        "for prov in PROVIDERS:\n",
        "    for pr in PROMPTS:\n",
        "        try:\n",
        "            llm = get_llm(prov)\n",
        "            res = run_benchmark(llm, prompt_name=pr, slices=SLICES)\n",
        "            m = score_binary(res)\n",
        "            f1 = f1_binary(m)\n",
        "            rows.append((prov, pr, m.accuracy, f1, m.unclear))\n",
        "        except Exception as e:\n",
        "            rows.append((prov, pr, float(\"nan\"), float(\"nan\"), str(e)))\n",
        "\n",
        "print(f\"{'Provider':<12} {'Prompt':<22} {'Accuracy':>10} {'F1':>8} {'Unclear':>8}\")\n",
        "print(\"-\" * 64)\n",
        "for prov, pr, acc, f1, unclear in rows:\n",
        "    acc_s = f\"{acc:.2%}\" if isinstance(acc, float) and acc == acc else \"N/A\"\n",
        "    f1_s = f\"{f1:.2%}\" if isinstance(f1, float) and f1 == f1 else \"N/A\"\n",
        "    u_s = str(unclear) if isinstance(unclear, int) else str(unclear)[:12]\n",
        "    print(f\"{prov:<12} {pr:<22} {acc_s:>10} {f1_s:>8} {u_s:>12}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
